{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity : Word2Vec embeddings\n",
    "\n",
    "In the section about Text Classification, we used various approaches to embed the sentences to classify. Some of the reference techniques involved statistical modelling (e.g. BOW, TFIDF), and others involved learning representations for words.\n",
    "\n",
    "In this activity, we propose to implement Word2Vec embeddings in PyTorch (during the previous activity we used the gensim implementation of the algorithm)\n",
    "\n",
    "## 1. Corpus\n",
    "\n",
    "We will work on a very simple corpus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'louis est roi de france',\n",
    "    'marie est reine de france',\n",
    "    'louis est un homme',\n",
    "    'marie est une femme',\n",
    "    'paris est la capitale de la france',\n",
    "    'bruxelles est la capitale de la belgique',\n",
    "    'tokyo est la capitale du japon',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same implementation as what we did earlier, we propose to preprocess the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "\n",
    "class FrenchTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [unidecode.unidecode(t) for t in word_list]\n",
    "\n",
    "#Tokenizing sentences\n",
    "tok=FrenchTokenizer()\n",
    "text_for_word2vec=[tok(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocabulary modelling : assigning indexes to words\n",
    "vocabulary = []\n",
    "for sentence in text_for_word2vec:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(\"vocabulary size : \", vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2Vec architecture relies on predicting the tokens surrounding a given one (or vice versa). It is composed of a simple auto-encoder using one-hot encodings of the tokens as inputs and outputs, and a hidden layer of a chosen size that will represent the embedding.\n",
    "\n",
    "![skipgram](img/word2vecskipgram.png)\n",
    "\n",
    "Because input output vectors are one-hot encodings of the tokens, their dimensionnality is the size of the corpus (e.g : if there are 30 tokens in the vocabulary, the encoding will be of size 30, with each dimension representing the presence or absence of the token in the message)\n",
    "The hidden layer is of size N. Because of the auto-encoder structure, the hidden layer will be trained to constrain the information about tokens in a space of given dimensionnality.\n",
    "\n",
    "There are two approaches for Word2Vec :\n",
    "- the Continuous Bag-of-words (CBOW) appraoach, where the token is used to predict surrounding ones\n",
    "- the Skip-Gram approach, where context tokens are used to predict the central one\n",
    "\n",
    "In this exercice, we propose to implement the SkipGram approach. To train this model, we need token pairs (a context token and the central one).\n",
    "Token pairs are generated by going through all tokens and generating context tokens belonging to a window of a given size around the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 1\n",
    "\n",
    "idx_pairs = []\n",
    "# for each sentence\n",
    "for sentence in text_for_word2vec:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    # for each word, treated as center word\n",
    "    for center_word_pos in range(len(indices)):\n",
    "        # for each window position\n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            # make soure not jump out sentence\n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs)\n",
    "\n",
    "#Printing first 10 pairs\n",
    "for i in range(10):\n",
    "    print(idx2word[idx_pairs[i][0]], idx2word[idx_pairs[i][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these pairs to train the network, we need to encode them in a layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_layer(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "#exemple\n",
    "get_layer(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Exercice : \n",
    "    \n",
    "Complete the class below to implement the architecture of the Skipgram.\n",
    "The class will include a method get_wv to get the word vector (i.e. the hidden layer) for a given word\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETE HERE\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class skipgram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        #vocab size : vocabulary size (corresponding to input and output dimensions)\n",
    "        #embedding_dim : dimension of the embedding (hidden) layer\n",
    "        super(skipgram, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        return output\n",
    "    def get_wv(self,input):\n",
    "        #get the word vector for a given input\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = skipgram(vocab_size=vocabulary_size,embedding_dim=2)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    loss_val = 0\n",
    "    for center_word, target_word in idx_pairs:\n",
    "        optimizer.zero_grad()\n",
    "        input = get_layer(center_word)\n",
    "        target = get_layer(target_word)\n",
    "        output=model(input)\n",
    "        loss = loss_function(output, target)\n",
    "        loss_val += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epo%10==0:\n",
    "        print(f'Loss at epoch {epo}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the word embeddings. They could be used to represent individual tokens in a classification task, for instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {w : model.get_wv(get_layer(word2idx[w])) for w in vocabulary}\n",
    "\n",
    "print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
