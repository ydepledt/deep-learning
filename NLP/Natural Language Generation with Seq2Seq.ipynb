{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "# Natural Language Generation with Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "In this exercice, we will try out the approach of Natural Language Generation, using a Seq2Seq (sequence to sequence) architecture. We will train a system that translates basic English sentences into French. The data used for this example is a list of French sentences and their translation into English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "The Seq2Seq approach is the following :\n",
    "1. A LSTM network encodes the input sequence into state vectors, with a predefined dimensionality\n",
    "2. A decoder LSTM predicts the next token of a target sequence based on the beginning of the sequence. The initial state is given by the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*Ismhi-muID5ooWf3ZIQFFg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "## 1. Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDemZoLucy7A"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/SupaeroDataScience/deep-learning\n",
    "#!mv deep-learning/NLP/datasets ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BysHZiI8Zgy7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "hidden_size = 256  # Latent dimensionality of the encoding space.\n",
    "\n",
    "# Path to the data\n",
    "data_path = 'datasets/enfratexts.txt'\n",
    "\n",
    "#setting up device for use with pyTorch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device used is \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmq5TUFWZgy-"
   },
   "source": [
    "### 1.1 Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmq5TUFWZgy-"
   },
   "source": [
    "We vectorize the data using words as features. This means that we will first define a vocabulary containing all the words in our corpus. The sequences will then be vectorized as a sequence of ints, corresponding to the id a given word in the dictionnary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fmq5TUFWZgy-"
   },
   "source": [
    "This section defines a set of utilities to import data :\n",
    "- A class defining the characteristics of the languages imported (French and English), including the size of the vocabulary and the dictionnary mapping words to indexes.\n",
    "- Methods to import data, including a simple preprocessing that lowercases all the characters and removes special characters, in order to control the size of our dictionnay (see notebook 1). \n",
    "- Methods to select data from our dataset, useful for running iterations of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBBCa67Zgy_"
   },
   "outputs": [],
   "source": [
    "# Start and end sequence tokens\n",
    "Start_sentence_token = 1\n",
    "End_sentence_token = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FoBBCa67Zgy_"
   },
   "outputs": [],
   "source": [
    "#Class defining a language.\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}#Contains the index of each word in the dictionnary\n",
    "        self.word2count = {}#Contains the count of each word\n",
    "        self.index2word = {1: \"SOS\", 0: \"EOS\"} #Reverse lookup table for words\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBBCa67Zgy_"
   },
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "\n",
    "# Convert to ASCII (because of french sentences)\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Trim, lowercase sentences and remove special chracters except punctuation\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBBCa67Zgy_"
   },
   "outputs": [],
   "source": [
    "# Filter data to keep only some relevant pairs. In particular, to ensure that the system trains fast enough,\n",
    "# we define a max length for sequences and keep only sentences sentences that start the same.\n",
    "max_length = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return (len(p[0].split(' ')) < max_length and\n",
    "            len(p[1].split(' ')) < max_length and p[0].startswith(eng_prefixes))\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBBCa67Zgy_"
   },
   "outputs": [],
   "source": [
    "# Reading, Normalizing data\n",
    "def readLangs(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(data_path, encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Create language objects\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBBCa67Zgy_"
   },
   "outputs": [],
   "source": [
    "# Full pipeline for importing data :\n",
    "# Reads the files, and cleans data\n",
    "# Filters pairs of english/french sentences to keep only those that are short enough and start the same\n",
    "def prepareData(lang1, lang2):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Filtered to %s sentence pairs\" % len(pairs))\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Dictionnary size:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz8MTaCsZgzA"
   },
   "source": [
    "### 1.2 Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nV-Wz8miZgzB"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('english', 'francais')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0W-tfSeZgzB"
   },
   "source": [
    "### 1.3 Vectorizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vom6kp7NZgzC"
   },
   "source": [
    "Vectorizing data for use by our Neural Network is performed through the folowing steps :\n",
    "- Step 1 : using the dictionary, the sequence of words is turned into a sequence of indexes\n",
    "- Step 2 : we append the End of Sentence token. Then, the sequence of indexes is turned into a tensor for use by pyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRQL3PFwZgzC"
   },
   "outputs": [],
   "source": [
    "# STEP 1\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "# STEP 2\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(End_sentence_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "# Performing the two steps of vectorization on a pair of english/french sequences\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbdXK9p4ZgzD"
   },
   "source": [
    "## 2. Defining the Encoder-Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbdXK9p4ZgzD"
   },
   "source": [
    "### 2.1. Encoder\n",
    "\n",
    "Encoding is performed via a LSTM, whose state we will store to condition the decoding. \n",
    "Inputs are first embedded into fixed dimensional space, and then encoded by the lstm. We also keep the hidden states of the lstm, as we need to feed them to the encoder for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9Y3BtN8ZgzE"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embedding Layer\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        #LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):       \n",
    "        #Embedding the input\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        # We feed the embedded vector as well as the hidden states passed as argument into the lstm\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81Yb_yi_ZgzF"
   },
   "source": [
    "### 2.2. Decoder\n",
    "\n",
    "The decoder is meant to predict the next token of the target sentence, knowing the current token and the context vectors given by the encoder (hidden vectors).\n",
    "The context vectors ancode the input sequence that was given, and will condition all the prediction of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jk73X4srZgzG"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        #Embedding Layer\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        #LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        #Linear layer mapping to the output size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #Embedding the input and applying relu\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        # We feed the embedded vector as well as the context vector passed as argument into the lstm\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        # Softmax layer (probabilities of each token)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "## 3. Teacher Forcing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "Say we work with the following sequence :\n",
    "\n",
    "```\n",
    "Rien ne sert de courir il faut partir à point\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "We want to train a model that predicts the following word of the sequence based on the start of it. First, in order for the first word of the sequence and the end of it to be predicted, we need to add beginning and end tokens to the sequence. We decide to use \\t as the beginning token, and \\n as the end one :\n",
    "\n",
    "```\n",
    "\\t Rien ne sert de courir il faut partir à point \\n\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "When training the system, we start by inputing the \"\\t\" beginning token:\n",
    "\n",
    "```\n",
    "input: \n",
    "\\t\n",
    "prediction:\n",
    "sert\n",
    "```\n",
    "The untrained model generated \"sert\" where we expected \"Rien\". There are now two options to continue :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "#### Without forcing :\n",
    "\n",
    "We add the previous output, \"sert\", to the input sequence, and continue generating :\n",
    "\n",
    "```\n",
    "input: \n",
    "\\t sert\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAFQ5dbnZgy4"
   },
   "source": [
    "With this approach, the error will propagate and make the model much slower to learn. \n",
    "\n",
    "### With teacher forcing\n",
    "\n",
    "After computing error, we discard the output \"sert\", and replace it with the word that was actually expected (\"Rien\"). This is called *teacher forcing* :\n",
    "\n",
    "```\n",
    "input: \n",
    "\\t Rien\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUTsrT3GZgzG"
   },
   "source": [
    "## 4. Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some variables for keeping track of loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses = [] #Will hold all losses for plotting\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we randomly initialize our networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvVfS-dVZgzJ"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = Decoder(hidden_size, output_lang.n_words).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup optimizers\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#Prepare n_iter training data to run n-iter steps\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(1, n_iters + 1):\n",
    "    training_pair = training_pairs[iter - 1]\n",
    "\n",
    "    # Retrieve the next tensors for input and target\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    #Run one step of training\n",
    "        #initialize hidden and cell state of the encoder lstm randomly\n",
    "    encoder_hidden = torch.randn(1, 1, hidden_size).to(device)\n",
    "    encoder_cell = torch.randn(1, 1, hidden_size).to(device)\n",
    "\n",
    "    #zero out the gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    #Get length of input and target sequences\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    ## ENCODER\n",
    "    #initialize the output of the encoder to zero\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    #We pass each input token to the encoder. At each step, we retrive the output and the hidden/cell states,\n",
    "    #forming the context vector. The context vector is fed back to the encoder for the next step.\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei], (encoder_hidden,encoder_cell))\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    ## DECODER\n",
    "    #For the decoder, the input is initialized with a Start of Sequence token\n",
    "    decoder_input = torch.tensor([[Start_sentence_token]], device=device)\n",
    "\n",
    "    #The decoder states are initialized by passing the context vector from the encoder\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    \n",
    "    #We pass each target token to the decoder. We keep the hidden and cell states, that we will feed back to the\n",
    "    #decoder for the next step. However, the outputs are discarded, and the next input of the decoder is the target\n",
    "    #output (see teacher forcing above)\n",
    "    for di in range(target_length):\n",
    "        decoder_output, (decoder_hidden,decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    #Backward prop\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    #Return loss\n",
    "    loss = loss.item() / target_length\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    #Every few steps, we print the current status of training. We also store the loss for plotting\n",
    "    if iter % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('(iteration %d %d%%) loss = %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "#Plot learning curve at the end\n",
    "showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "esrnNy8MZgzG"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length=max_length):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY9rqbelZgzH"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kY9rqbelZgzH"
   },
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvVfS-dVZgzJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainIters(encoder, decoder, 2000, print_every=1000, plot_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-vq7PlCZgzJ"
   },
   "source": [
    "## 4.4. Inference\n",
    "\n",
    "For inference, the only difference with training is that we will continue to feed back the network's predictions to itself at each step, and stop only when we predict an end of sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10s0guciZgzJ"
   },
   "outputs": [],
   "source": [
    "def inference(encoder, decoder, sentence, max_length=max_length):\n",
    "    \n",
    "    with torch.no_grad(): #Freeze gradient\n",
    "        \n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        #Initialize the encoder hidden states\n",
    "        encoder_hidden = torch.randn(1, 1, hidden_size).to(device)\n",
    "        encoder_cell = torch.randn(1, 1, hidden_size).to(device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        ##ENCODER\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei],\n",
    "                                                     (encoder_hidden,encoder_cell))\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        #Initialize decoder input with a start of sentence token\n",
    "        decoder_input = torch.tensor([[Start_sentence_token]], device=device) \n",
    "\n",
    "        #Feed the encoder context vectors to the decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell=encoder_cell\n",
    "\n",
    "        decoded_words = [] #Will hold the decoded sequence (translation)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, (decoder_hidden,decoder_cell) = decoder(decoder_input, (decoder_hidden,decoder_cell))\n",
    "            \n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == End_sentence_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break #Stop if we predict an end of sentence token\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach() #Use the previously predicted token as the input for the next step\n",
    "\n",
    "        return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1UC71-MZgzK"
   },
   "source": [
    "We define a util function that will evaluate 10 random sentences from the train set and try to translate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NW1q3csZgzL"
   },
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words = inference(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQJ36oYTZgzM"
   },
   "outputs": [],
   "source": [
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhBQtl-7ZgzO"
   },
   "source": [
    "After only a few training steps, we observe that translations are often all identical. It would take more several thousands of training iterations (probably about 100 000) to reach a satisfying result with this system. There are two main reasons that can explain this problem :\n",
    "\n",
    "- First, the **use of teacher forcing**. Because we systematically correct the predictions of the network during training, the system has a tendency to learn to predict sentences that are grammatically correct, but would fail to learn the actual meaning. As an exercise, we could try to modfy the train function to not include teacher forcing. The flexibility of PyTorch also allows us to use teacher forcing sometimes, but not always.\n",
    "- Then, the **architecture of the network**. Here, because the context vectors fed into the decoder are the same for a given input sentence, this means that the encoder has the responsibility to learn representations for the input sequence ***in its entirety***. For longer sentences, this isquite unefficient. In the rest of this notebook, we will introduce the notion of attention, as a response to this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGl9HkcNZgzO"
   },
   "source": [
    "# 5. Using Attention\n",
    "\n",
    "A variant of the previous Seq2Seq system involves a mechanism mimicking **attention**. With this mechanism, instead of conditionning the prediction using the raw context vector, we will apply attention weights In practice, this means that the system will learn to \"use\" some parts of the encoder output more than others when predicting a sequence.\n",
    "\n",
    "## 5.1. Implementing a decoder with attention\n",
    "\n",
    "In this paradigm, encoding is performed in the same way as previously. \n",
    "\n",
    "In the previous system, the decoder predicted the next token of the target sequence based on the known one. This prediction was conditioned by the encoded context vectors.\n",
    "The difference here is that instead of directly predicting the next token from the input, the decoder will first predict where to focus its attention on the encoded sequence, and will then use this attended vector as well as the input topredict what the next token is.\n",
    "\n",
    "Let's write the new version of our decoder :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dh3asvuIZgzO"
   },
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=max_length):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p #dropout probability\n",
    "        self.max_length = max_length\n",
    "\n",
    "        #Embedding layers for the decoder input\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        #First, the input (target sequence) is embedded. Some weights are randomly zeroed out to facilitate\n",
    "        #learning with the attention mechanism\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        #Attention is computed by combining the context vectors and the embedded input (french sequence)\n",
    "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
    "        #Attention is applied on the encoded original sentence (in english)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        #We retrieve the embedded input and the context vector (with attention applied), and combine the two tensors\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        \n",
    "        #The attended part of the input is fed into the lstm, conditioned by the hidden and cell states\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        #Retrieve token probabilities\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        #In addition to the output and hidden states that are necessary for iterating, we return the attention\n",
    "        #weights, that will provide some form of explainability\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coRcS9OqZgzP"
   },
   "source": [
    "Because the input and output of the forward pass of this decoder is not the same as before (we added the attention weights and encoder output), we need to adapt the train, trainIter and inference functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbU7zescZgzP"
   },
   "outputs": [],
   "source": [
    "def train_with_attention(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=max_length):\n",
    "\n",
    "    encoder_hidden = torch.randn(1, 1, hidden_size).to(device)\n",
    "    encoder_cell = torch.randn(1, 1, hidden_size).to(device)\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei], (encoder_hidden,encoder_cell))\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[Start_sentence_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    \n",
    "    # THE ONLY CHANGES OCCURS HERE, AS WE NEED TO ADD encoder_output AS AN INPUT AND attention AS AN OUTPUT\n",
    "    for di in range(target_length):\n",
    "        decoder_output, (decoder_hidden,decoder_cell), decoder_attention  = decoder(decoder_input, (decoder_hidden,decoder_cell), encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length\n",
    "\n",
    "#Iterations of training\n",
    "def trainIters_with_attention(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0\n",
    "    plot_loss_total = 0 \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        #THE ONLY CHANGE HAPPENS HERE, AS WE NEED TO CALL THE TRAIN_WITH_ATTENTION FUNCTION\n",
    "        loss = train_with_attention(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('(iteration %d %d%%) loss = %.4f' % (iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n",
    "  \n",
    "#evaluation function\n",
    "def inference_with_attention(encoder, decoder, sentence, max_length=max_length):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = torch.randn(1, 1, hidden_size).to(device)\n",
    "        encoder_cell = torch.randn(1, 1, hidden_size).to(device)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, (encoder_hidden,encoder_cell) = encoder(input_tensor[ei],\n",
    "                                                     (encoder_hidden,encoder_cell))\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[Start_sentence_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        # ONLY CHANGE IS HERE\n",
    "        for di in range(max_length):\n",
    "            decoder_output, (decoder_hidden,decoder_cell), decoder_attention = decoder(\n",
    "                decoder_input, (decoder_hidden,decoder_cell), encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == End_sentence_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n",
    "\n",
    "#evaluate several sentences picked randomly    \n",
    "def evaluateRandomly_with_attention(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = inference_with_attention(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0avZbgSbZgzQ"
   },
   "source": [
    "Let's train and test this system !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEyje9SHZgzQ"
   },
   "outputs": [],
   "source": [
    "encoder_for_attn = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder = AttnDecoder(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "\n",
    "trainIters_with_attention(encoder_for_attn, attn_decoder, 1000, print_every=100,plot_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYV86QfEZgzR"
   },
   "outputs": [],
   "source": [
    "evaluateRandomly_with_attention(encoder_for_attn, attn_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPE18-pZZgzR"
   },
   "source": [
    "## 5.2. Visualize attention\n",
    "\n",
    "One of the advantages of using attention is that it provides explainability of the output. As we have the ability to see what part of the sequence was attended to when predicting a given output token, it provides an explanation of correspondences between target and input tokens.\n",
    "\n",
    "In this section, we will write a function to visualize attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARnr3xm8ZgzR"
   },
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = inference_with_attention(\n",
    "        encoder_for_attn, attn_decoder, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGz5I1S7ZgzS"
   },
   "outputs": [],
   "source": [
    "evaluateAndShowAttention(\"i am not making any plans .\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
